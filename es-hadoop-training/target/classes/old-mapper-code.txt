package com.statefarm.es_hadoop;
import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class StubMapper extends Mapper<LongWritable, Text, Text, MapWritable> {
//public class StubMapper extends Mapper<LongWritable, Text, Text, MapWritable> {
	private Text word = new Text();
	private LongWritable count = new LongWritable();
	private MultipleOutputs<Text, LongWritable> outputFormat;
	 Logger logger = LoggerFactory.getLogger(StubMapper.class);
	
  @Override
  public void map(LongWritable key, Text value, Context context)
      throws IOException, InterruptedException {
	  // value is tab separated values: word, year, occurrences, #books, #pages
	  // we project out (word, occurrences) so we can sum over all years
	  String[] split = value.toString().split("\t+");
	  
	  word.set(split[0]);
	  
	  if (split != null && split.length > 2) {
		  try { 
			  count.set(Long.parseLong(split[2]));
			  
			  MapWritable doc = new MapWritable();
			  doc.put(new Text("word"), word);
			  doc.put(new Text("count"), count);
			  
//			  context.write(word, doc);
			  context.write(word, count);
			  
			  logger.warn("In Mapper just wrote: " + word + ": " + count);
			  
//			  outputFormat.write("default",  word, count);
		  } catch (NumberFormatException e) {
			  // cannot parse - ignore
			  
			  // we could take another action, such as incrementing a Mapreduce counter to 
			  // track how many liines it affects (see the getCounter() method on Context for details
		  }
	  }
  }
  
//  @Override
//  public void setup(Context context) {
//	  outputFormat = new MultipleOutputs<Text, LongWritable>(context);
//  }
}
